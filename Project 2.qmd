---
title: "Project 2"
---

**Name:** Da Yea Song (dsong13\@jhmi.edu; dsong13)

## Set Up

```{r}
library(dplyr)
library(purrr)
library(tidyr)
library(tidycensus)
library(tidyverse)
```

## Part 1

1.  Choose a question to investigate. Describe what is the question you aim to answer with the data and what you want to visualize.

    **How was the income gap between men and women varied across US states over the past three years?**

2.  Extract data from the `tidycensus` API. Use at least three different calls to the `tidycensus` API to extract out different datasets. For example, these could be across years, locations, or variables.

    ```{r}
    # Check variables 
    variables_2021 <- load_variables(2021, "acs1", cache = TRUE)
    head(variables_2021)
    ```

    B20017_002 male median income; B20017_005 female median income

    ```{r}

    # Call 1: Median Income by Sex for 2021
    income_2021 <- get_acs(
      geography = "state",
      variables = c(male_income = "B20017_002", female_income = "B20017_005"),
      year = 2021,
      survey = "acs1"
    )

    # Call 2: Median Income by Sex for 2021
    income_2022 <- get_acs(
      geography = "state",
      variables = c(male_income = "B20017_002", female_income = "B20017_005"),
      year = 2022,
      survey = "acs1"
    )

    # Call 3: Median Income by Sex for 2023
    income_2023 <- get_acs(
      geography = "state",
      variables = c(male_income = "B20017_002", female_income = "B20017_005"),
      year = 2023,
      survey = "acs1"
    )
    ```

3.  Clean the data. Include some form of data wrangling and data visualization using packages such as `dplyr` or `tidyr`. Other packages that might be helpful to you include `lubridate`, `stringr`, and `forcats`. You **must use at least two functions from `purrr`**.

    ```{r}
    # Step 1: Add year column using mutate (dplyr)
    income_2021 <- income_2021 %>% mutate(year = 2021)
    income_2022 <- income_2022 %>% mutate(year = 2022)
    income_2023 <- income_2023 %>% mutate(year = 2023)
    ```

    ```{r}
    # Step 2: Combine into one dataset using map_dfr (purrr)
    income_data <- list(income_2021, income_2022, income_2023)

    income_total <- map_dfr(income_data, ~ .x)

    print(income_total)
    ```

    ```{r}
    # Step 3: Remove moe columns using select (dplyr)
    income_total <- income_total %>%
      select(-moe)

    print(income_total)
    ```

    ```{r}
    # Step 3: Reorganize data using pivot_wider (tidyr)
    income_total <- income_total %>%
      pivot_wider(
        names_from = variable,
        values_from = c(estimate),
        names_glue = "{variable}_{.value}"
      ) %>%
      rename(
        male_income = male_income_estimate,
        female_income = female_income_estimate,
      )

    print(income_total)
    ```

    ```{r}
    # Step 4: Calculate income difference between sexes using pmap_dbl (purr)
    income_total <- income_total %>%
      mutate(income_difference = pmap_dbl(
        list(male_income, female_income),
        ~ ..1 - ..2
      ))

    print(income_total)
    ```

    ```{r}
    # Step 5: Reorder the table by year and income difference using arrange (dplyr)
    income_total <- income_total %>%
      arrange(year, income_difference)

    print(income_total)
    ```

4.  Visualize the data. Create data visualizations of your choice. However, your analysis should include at least three plots with you using at least two different `geom_*()` functions from `ggplot2` (or another package with `geom_*()` functions).

5.  Report your findings. Provide a paragraph summarizing your methods and key findings. Include any limitations or potential biases in pulling data from the API or the analysis. Be sure to comment and organize your code so is easy to understand what you are doing.

## Part 2

In this part, you and your partner will use the `rvest` package to scrape data from a website, wrangle and analyze the data, and summarize your findings.

1.  Choose a website to scrape. Select a website with structured data in HTML tables or well-defined sections. Some examples could include:

    -   A movie database like IMDb or Rotten Tomatoes (scraping movie titles, ratings, release years, etc.)

    -   A job listing site like Indeed or LinkedIn (scraping job titles, companies, and locations)

    -   A sports statistics site like ESPN or Baseball Reference (scraping team statistics, player info, etc.)

2.  Extract data with `rvest`. Here, you will want to identify the specific HTML elements or CSS selectors containing the data. Then, use `rvest` functions like `read_html()`, `html_elements()`, and `html_text()` or `html_table()` to retrieve the data.

3.  Clean the data. Next, perform some basic wrangling, such as remove extra whitespace, handle missing values, and convert data types as needed. You might find the functions from `dplyr` or `tidyr` useful for any additional transformations, such as renaming columns, filtering rows, or creating new variables.

4.  Analyze the data. Perform a simple analysis of your choice. For example, you could

    -   Count how many times specific words or themes appear.

    -   Create a summary statistic (e.g., average rating, job salary, team win percentage).

    -   Create a data visualization (e.g., bar chart, histogram) of an interesting metric.

5.  Report your findings. Provide a paragraph summarizing your methods and key findings. Include any limitations or potential biases in your scraping or analysis. Be sure to comment and organize your code so is easy to understand what you are doing.
